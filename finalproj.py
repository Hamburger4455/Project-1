# -*- coding: utf-8 -*-
"""finalproj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yGH4HpWtTUkPW3Jfd_4N52T-p4tLRdfi
"""

from keras.models import load_model
import tensorflow as tf
import cv2  # Install opencv-python
import numpy as np
import os
from google.colab import drive
from keras.layers import DepthwiseConv2D as KerasDepthwiseConv2D
import matplotlib.pyplot as plt  # Import for image display

# Mount Google Drive
drive.mount('/content/drive')

# Disable scientific notation for clarity
np.set_printoptions(suppress=True)

# Define custom DepthwiseConv2D to avoid 'groups' parameter issue
class CustomDepthwiseConv2D(KerasDepthwiseConv2D):
    def __init__(self, **kwargs):
        # Remove 'groups' from the parameters and pass the rest
        if 'groups' in kwargs:
            del kwargs['groups']
        super().__init__(**kwargs)

# Load the model with custom DepthwiseConv2D
model = load_model("/content/drive/Shareddrives/proj2/proj2/keras_model.h5", compile=False,
                   custom_objects={'DepthwiseConv2D': CustomDepthwiseConv2D})

# Load the labels from the converted_keras folder
class_names = open("/content/drive/Shareddrives/proj2/proj2/labels.txt", "r").readlines()

# Define the path to your test images directory
test_images_dir = "/content/drive/MyDrive/test/mixed dataset"  # Adjust this to your test image folder's path
test_images = os.listdir(test_images_dir)  # List all files in the folder

# Function to check if a file is an image based on file extension
def is_image(file_path):
    valid_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif']
    # Ensure the check is case-insensitive by converting to lowercase
    return any(file_path.lower().endswith(ext) for ext in valid_extensions)

# Loop through each image in the test dataset
for image_name in test_images:
    # Construct the full path to the image
    image_path = os.path.join(test_images_dir, image_name)

    # Skip non-image files or directories and log the reason
    if os.path.isdir(image_path):
        print(f"Skipping directory: {image_name}")
        continue
    elif not is_image(image_path):
        print(f"Skipping non-image file (invalid extension): {image_name}")
        continue

    # Read the image
    image = cv2.imread(image_path)

    # Check if the image was loaded correctly
    if image is None:
        print(f"Error reading image (possibly corrupted): {image_name}")
        continue

    # Resize the raw image into (224, 224) pixels (same size as the model input)
    image_resized = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)

    # Normalize the image array (same as the model expects)
    image_array = np.asarray(image_resized, dtype=np.float32).reshape(1, 224, 224, 3)
    image_array = (image_array / 127.5) - 1

    # Predict with the model
    prediction = model.predict(image_array)
    index = np.argmax(prediction)

    # Get class name and remove the numeric part if present (e.g., '0 Hand' -> 'Hand')
    class_name = class_names[index].strip().split(maxsplit=1)[-1].lower()  # Extract and convert to lowercase
    confidence_score = prediction[0][index]

    # Print the result
    print(f"Image: {image_name}")
    print(f"Class: {class_name}")  # Now it will print just the class name, e.g., 'hand'
    print(f"Confidence Score: {np.round(confidence_score * 100)}%")
    print("-" * 30)

    # Display the image with the prediction
    plt.imshow(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for proper color display
    plt.title(f'Prediction: {class_name} ({np.round(confidence_score * 100)}%)')
    plt.axis('off')  # Hide axes for cleaner display
    plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Load MNIST dataset
(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()

# Normalize data
x_train, x_val = x_train / 255.0, x_val / 255.0

# Create model
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Add EarlyStopping and ReduceLROnPlateau callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train the model and store the training history
history = model.fit(x_train, y_train, epochs=30, validation_data=(x_val, y_val))

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()